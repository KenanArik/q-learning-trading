{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rand\n",
    "from copy import deepcopy\n",
    "\n",
    "class QLearner(object):\n",
    "\n",
    "    def __init__(self, num_states=100, num_actions=4, alpha=0.2,\n",
    "        gamma=0.9, rar=0.5, radr=0.99, dyna=0, verbose=False):\n",
    "        \"\"\"The constructor QLearner() reserves space for keeping track of Q[s, a] for \n",
    "        the number of states and actions. It initializes Q[] with all zeros.\n",
    "        Parameters:\n",
    "        num_states: int, the number of states to consider\n",
    "        num_actions: int, the number of actions available\n",
    "        alpha: float, the learning rate used in the update rule. \n",
    "               Should range between 0.0 and 1.0 with 0.2 as a typical value\n",
    "        gamma: float, the discount rate used in the update rule. \n",
    "               Should range between 0.0 and 1.0 with 0.9 as a typical value.\n",
    "        rar: float, random action rate. The probability of selecting a random action \n",
    "             at each step. Should range between 0.0 (no random actions) to 1.0 \n",
    "             (always random action) with 0.5 as a typical value.\n",
    "        radr: float, random action decay rate, after each update, rar = rar * radr. \n",
    "              Ranges between 0.0 (immediate decay to 0) and 1.0 (no decay). Typically 0.99.\n",
    "        dyna: int, conduct this number of dyna updates for each regular update. \n",
    "              When Dyna is used, 200 is a typical value.\n",
    "        verbose: boolean, if True, your class is allowed to print debugging \n",
    "                 statements, if False, all printing is prohibited.\n",
    "        \"\"\"        \n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.rar = rar\n",
    "        self.radr = radr\n",
    "        self.dyna = dyna\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Keep track of the latest state and action which are initialized to 0\n",
    "        self.s = 0\n",
    "        self.a = 0\n",
    "        \n",
    "        # Initialize a Q table which records and updates Q value for\n",
    "        # each action in each state\n",
    "        self.Q = np.zeros(shape=(num_states, num_actions))\n",
    "        # Keep track of the number of transitions from s to s_prime for when taking \n",
    "        # an action a when doing Dyna-Q\n",
    "        self.T = {}\n",
    "        # Keep track of reward for each action in each state when doing Dyna-Q\n",
    "        self.R = np.zeros(shape=(num_states, num_actions))\n",
    "\n",
    "    def query_set_state(self, s):\n",
    "        \"\"\"Find the next action to take in state s. Update the latest state and action \n",
    "        without updating the Q table. Two main uses for this method: 1) To set the  \n",
    "        initial state, and 2) when using a learned policy, but not updating it.\n",
    "        Parameters:\n",
    "        s: The new state\n",
    "        \n",
    "        Returns: The selected action to take in s\n",
    "        \"\"\"\n",
    "        if rand.uniform(0.0, 1.0) < self.rar:\n",
    "            action = rand.randint(0, self.num_actions - 1)\n",
    "        else:\n",
    "            action = self.Q[s, :].argmax()\n",
    "        self.s = s\n",
    "        self.a = action\n",
    "        if self.verbose: \n",
    "            print (\"s =\", s,\"a =\",action)\n",
    "        return action\n",
    "\n",
    "    def query(self, s_prime, r):\n",
    "        \"\"\"Find the next action to take in state s_prime. Update the latest state \n",
    "        and action and the Q table. Update rule:\n",
    "        Q'[s, a] = (1 - α) · Q[s, a] + α · (r + γ · Q[s', argmax a'(Q[s', a'])]).\n",
    "        Parameters:\n",
    "        s_prime: int, the new state\n",
    "        r: float, a real valued immediate reward for taking the previous action\n",
    "        \n",
    "        Returns: The selected action to take in s_prime\n",
    "        \"\"\"\n",
    "        # Update the Q value of the latest state and action based on s_prime and r\n",
    "        self.Q[self.s, self.a] = (1 - self.alpha) * self.Q[self.s, self.a] \\\n",
    "                                    + self.alpha * (r + self.gamma \n",
    "                                    * self.Q[s_prime, self.Q[s_prime, :].argmax()])\n",
    "\n",
    "        # Implement Dyna-Q\n",
    "        if self.dyna > 0:\n",
    "            # Update the reward table\n",
    "            self.R[self.s, self.a] = (1 - self.alpha) * self.R[self.s, self.a] \\\n",
    "                                        + self.alpha * r\n",
    "            \n",
    "            if (self.s, self.a) in self.T:\n",
    "                if s_prime in self.T[(self.s, self.a)]:\n",
    "                    self.T[(self.s, self.a)][s_prime] += 1\n",
    "                else:\n",
    "                    self.T[(self.s, self.a)][s_prime] = 1\n",
    "            else:\n",
    "                self.T[(self.s, self.a)] = {s_prime: 1}\n",
    "            \n",
    "            Q = deepcopy(self.Q)\n",
    "            for i in range(self.dyna):\n",
    "                s = rand.randint(0, self.num_states - 1)\n",
    "                a = rand.randint(0, self.num_actions - 1)\n",
    "                if (s, a) in self.T:\n",
    "                    # Find the most common s_prime as a result of taking a in s\n",
    "                    s_pr = max(self.T[(s, a)], key=lambda k: self.T[(s, a)][k])\n",
    "                    # Update the temporary Q table\n",
    "                    Q[s, a] = (1 - self.alpha) * Q[s, a] \\\n",
    "                                + self.alpha * (self.R[s, a] + self.gamma \n",
    "                                * Q[s_pr, Q[s_pr, :].argmax()])\n",
    "            # Update the Q table of the learner once Dyna-Q is complete\n",
    "            self.Q = deepcopy(Q)\n",
    "        \n",
    "        # Find the next action to take and update the latest state and action\n",
    "        a_prime = self.query_set_state(s_prime)\n",
    "        self.rar *= self.radr\n",
    "        if self.verbose: \n",
    "            print (\"s =\", s_prime,\"a =\",a_prime,\"r =\",r)\n",
    "        return a_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
